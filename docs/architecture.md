# üèóÔ∏è Architecture

This document provides a deep dive into DockAI's architecture, explaining not just how the components work, but **why** they were designed this way. Understanding the architecture helps you extend DockAI, debug issues, and make the most of its customization capabilities.

---

## üìã Table of Contents

1. [Design Philosophy](#design-philosophy)
2. [High-Level Architecture](#high-level-architecture)
3. [The LangGraph State Machine](#the-langgraph-state-machine)
4. [The 8 AI Agents](#the-8-ai-agents)
5. [The AgentContext Pattern](#the-agentcontext-pattern)
6. [Data Flow](#data-flow)
7. [The Self-Correcting Loop](#the-self-correcting-loop)
8. [Interfaces](#interfaces)
9. [Directory Structure](#directory-structure)
10. [Design Decisions Explained](#design-decisions-explained)

---

## Design Philosophy

Before examining the architecture, it's important to understand the principles that guided its design:

### 1. First-Principles Over Templates

**Problem**: Template-based generators fail when projects don't match predefined patterns.

**Solution**: Use AI to reason about requirements from first principles, just like a human DevOps engineer would.

### 2. Validation Over Trust

**Problem**: Generated code might look correct but fail in practice.

**Solution**: Actually build and test every Dockerfile. Real-world validation catches issues static analysis cannot.

### 3. Learning Over Failing

**Problem**: Complex projects often don't work on the first try.

**Solution**: When validation fails, analyze why and try again with that knowledge. This is how humans solve problems.

### 4. Separation of Concerns

**Problem**: Monolithic systems are hard to customize and debug.

**Solution**: Split the problem into specialized agents, each responsible for one aspect. This enables targeted customization and clearer debugging.

### 5. Unified Context

**Problem**: Information needed by one agent might be scattered across multiple sources.

**Solution**: All agents share a single `AgentContext` containing all relevant information. This ensures consistency and reduces redundant computation.

---

## High-Level Architecture

DockAI is built on three main pillars:

```mermaid
flowchart TB
    subgraph Interfaces["üñ•Ô∏è User Interfaces"]
        cli["CLI\n(dockai build)"]
        action["GitHub Action\n(itzzjb/dockai@v3)"]
        mcp["MCP Server\n(mcp_server.py)"]
    end
    
    subgraph Workflow["‚öôÔ∏è LangGraph Workflow (graph.py)"]
        scan["scan_node"] --> analyze["analyze_node"] --> read["read_files_node"]
        read --> blueprint["blueprint_node"] --> generate["generate_node"]
        generate --> review["review_node"] --> validate["validate_node"]
        
        validate -->|"should_retry"| reflect["reflect_node"]
        reflect --> increment["increment_retry"]
        increment -->|"minor fix"| generate
        increment -->|"strategy change"| blueprint
        increment -->|"wrong assumptions"| analyze
        validate -->|"end"| done["END"]
    end
    
    subgraph Services["üîß Core Services"]
        llm["LLM Providers\n(llm_providers.py)\nOpenAI, Azure, Gemini,\nAnthropic, Ollama"]
        docker["Docker Engine\n(validator.py)"]
        registry["Registry Client\n(registry.py)"]
        security["Security Scanners\n(Trivy, Hadolint)"]
    end
    
    cli --> Workflow
    action --> Workflow
    mcp --> Workflow
    
    Workflow --> llm
    Workflow --> docker
    Workflow --> registry
    Workflow --> security
```

### Layer 1: User Interfaces

Three ways to interact with DockAI, all using the same core workflow:

- **CLI**: For developers running locally (`dockai build .`)
- **GitHub Action**: For CI/CD automation
- **MCP Server**: For AI assistants like Claude Desktop

### Layer 2: LangGraph Workflow

The heart of DockAI‚Äîa state machine that orchestrates the agents. Built on [LangGraph](https://langchain-ai.github.io/langgraph/), which provides:

- Deterministic execution flow
- State persistence across nodes
- Conditional branching (for retry loops)
- Streaming and observability

### Layer 3: Core Services

Supporting services that agents use:

- **LLM Providers**: OpenAI, Azure, Gemini, Anthropic, Ollama
- **Docker Engine**: Building and running containers
- **Registry Client**: Validating image tags exist
- **Security Scanners**: Trivy and Hadolint integration

---

## The LangGraph State Machine

### Why LangGraph?

DockAI's workflow isn't a simple linear pipeline‚Äîit has loops, conditionals, and shared state. LangGraph provides:

1. **State Management**: TypedDict state shared across all nodes
2. **Conditional Edges**: "If validation fails, go to Reflector"
3. **Compile-Time Validation**: Catches workflow errors before runtime
4. **Observability**: Built-in tracing and debugging support

### Workflow Definition

The workflow is defined in `src/dockai/workflow/graph.py`:

```python
from langgraph.graph import StateGraph, END, START

def create_graph() -> CompiledStateGraph:
    """Create the LangGraph workflow for Dockerfile generation."""
    
    workflow = StateGraph(GraphState)
    
    # Add all nodes
    workflow.add_node("scan", scan_node)
    workflow.add_node("analyze", analyze_node)
    workflow.add_node("read_files", read_files_node)
    workflow.add_node("blueprint", blueprint_node)
    workflow.add_node("generate", generate_node)
    workflow.add_node("review", review_node)
    workflow.add_node("validate", validate_node)
    workflow.add_node("reflect", reflect_node)
    workflow.add_node("increment_retry", increment_retry)  # Simple state updater
    
    # Linear flow through discovery and generation
    workflow.add_edge(START, "scan")
    workflow.add_edge("scan", "analyze")
    workflow.add_edge("analyze", "read_files")
    workflow.add_edge("read_files", "blueprint")
    workflow.add_edge("blueprint", "generate")
    workflow.add_edge("generate", "review")
    
    # Conditional edge after security review
    workflow.add_conditional_edges(
        "review",
        check_security,
        {"validate": "validate", "reflect": "reflect", "end": END}
    )
    
    # Conditional edge after validation
    workflow.add_conditional_edges(
        "validate",
        should_retry,
        {"end": END, "reflect": "reflect"}
    )
    
    # Reflect always leads to increment_retry
    workflow.add_edge("reflect", "increment_retry")
    
    # After incrementing, decide where to loop back
    workflow.add_conditional_edges(
        "increment_retry",
        check_reanalysis,
        {"blueprint": "blueprint", "generate": "generate", "analyze": "analyze"}
    )
    
    return workflow.compile()
```

### The GraphState

All state is stored in a TypedDict:

```python
class GraphState(TypedDict):
    # Input
    repo_path: str
    
    # Discovery results
    file_tree: List[str]
    analysis_result: Optional[AnalysisResult]
    file_contents: str
    
    # Blueprint results (Plan + Runtime Config)
    current_plan: Optional[PlanningResult]
    health_result: Optional[HealthEndpointDetectionResult]
    readiness_result: Optional[ReadinessPatternResult]
    
    # Generation state
    dockerfile_content: str
    thought_process: str
    
    # Validation state
    review_result: Optional[SecurityReviewResult]
    validation_result: Optional[ValidationResult]
    
    # Retry state
    retry_count: int
    retry_history: List[RetryAttempt]
    reflection: Optional[ReflectionResult]
    
    # Output
    final_dockerfile: str
    token_usage: Dict[str, int]
```

---

## The 8 AI Agents

DockAI uses specialized agents instead of one general-purpose agent. This design provides:

- **Better results**: Each agent has a focused prompt optimized for its task
- **Cost optimization**: Simple tasks use faster/cheaper models
- **Easier customization**: Override one agent without affecting others
- **Clearer debugging**: Know exactly which agent caused an issue

### Agent Overview

| Agent | Purpose | Model Type | Why This Split? |
|-------|---------|------------|-----------------|
| **Analyzer** | Detect tech stack | Fast | Simple pattern matching, doesn't need reasoning power |
| **Architect (Blueprint)** | Create build strategy & runtime config | Powerful | Strategic planning requires deep reasoning and context |
| **Generator** | Write Dockerfile | Powerful | Needs to synthesize complex information |
| **Generator (Iterative)** | Fix failed Dockerfile | Powerful | Debugging requires deep reasoning |
| **Reviewer** | Security audit | Fast | Rule-based checks with AI enhancement |
| **Reflector** | Analyze failures | Powerful | Root cause analysis is complex |
| **Error Analyzer** | Classify errors | Fast | Error categorization is straightforward |
| **Iterative Improver** | Apply specific fixes | Powerful | Code modification needs precision |

### Agent Details

#### 1. Analyzer Agent

**Location**: `src/dockai/agents/analyzer.py`

**Input**: File tree (list of file paths)

**Output**: `AnalysisResult` with:
- Detected stack (e.g., "Python 3.11 with FastAPI")
- Project type ("service" or "script")
- Suggested base image
- Files to read for more context
- Build and start commands

**Why it exists**: Before we can generate a Dockerfile, we need to understand what we're containerizing. The Analyzer looks at file patterns and names to deduce the technology stack.

**Example prompt snippet**:
```
You are a project analyzer. Given the file tree below, determine:
1. The primary programming language and framework
2. Whether this is a long-running service or a script
3. What base image would be appropriate
4. Which files contain critical information (dependencies, entry points)

File tree:
- app.py
- requirements.txt
- templates/
- static/
...
```

#### 2. Architect Agent (Blueprint)

**Location**: `src/dockai/agents/agent_functions.py`

**Input**: Analysis results, file contents, retry history

**Output**: `BlueprintResult` with:
- `PlanningResult`: Build strategy, base image, multi-stage decision
- `RuntimeConfigResult`: Health endpoints, readiness patterns, ports

**Why it exists**: This agent combines strategic planning ("How do we build this?") with runtime configuration detection ("How do we run this?"). By combining these steps, we reduce token usage and latency while ensuring the build strategy aligns with runtime requirements.

**Example considerations**:
- "This is a Go project, so we use multi-stage build (Plan) and look for `http.ListenAndServe` (Runtime Config)"
- "Detected `/health` endpoint in `app.py`, so we'll add a HEALTHCHECK instruction"

#### 3. Generator Agent

**Location**: `src/dockai/agents/generator.py`

**Input**: Analysis, file contents, blueprint, retry history, verified tags

**Output**: Complete Dockerfile content

**Why it exists**: This is the core task‚Äîactually writing the Dockerfile. It synthesizes all the gathered context into a working Dockerfile.

**What it considers**:
- Base image selection and tag verification
- Dependency installation with proper caching
- Security hardening (non-root user, minimal base)
- Health checks based on detected endpoints
- Build optimization (layer ordering, multi-stage)

#### 4. Reviewer Agent

**Location**: `src/dockai/agents/reviewer.py`

**Input**: Generated Dockerfile, project context

**Output**: `SecurityReviewResult` with:
- Is the Dockerfile secure? (boolean)
- List of issues found with severity
- Optionally, a fixed version

**Why it exists**: AI-generated code can have security issues. The Reviewer specifically looks for:
- Running as root
- Exposed secrets
- Unnecessary packages
- Vulnerable base images
- Missing security configurations

#### 5. Reflector Agent

**Location**: `src/dockai/agents/agent_functions.py`

**Input**: Error message, container logs, previous Dockerfile, retry history

**Output**: `ReflectionResult` with:
- Error classification
- Root cause identification
- Recommended strategy for next attempt
- Specific fixes to apply
- Lessons learned

**Why it exists**: When validation fails, we need to understand why before trying again. The Reflector performs root cause analysis and provides actionable guidance for the next attempt.

**Example reflection**:
```
Error: "pg_config executable not found"

Root Cause: The Dockerfile is trying to install psycopg2 from source, 
which requires PostgreSQL development headers.

Recommended Fix: Install postgresql-dev (Alpine) or libpq-dev (Debian)
before pip install.

Lessons Learned:
- Python packages with C extensions need build dependencies
- Consider using psycopg2-binary for simpler installation
```

---

## The AgentContext Pattern

### Why AgentContext?

Originally, each agent had different function signatures:

```python
# Old approach - inconsistent signatures
analyze_repo_needs(file_tree, custom_instructions)
generate_dockerfile(analysis, file_contents, plan, retry_history, ...)
reflect_on_failure(error, dockerfile, logs, history, ...)
```

This caused problems:
- Adding new context required changing many signatures
- Easy to forget passing required information
- No type safety across agents

### The Solution

All agents now receive an `AgentContext` dataclass:

```python
@dataclass
class AgentContext:
    """Unified context passed to all AI agents."""
    
    # Project information
    file_tree: List[str] = field(default_factory=list)
    file_contents: str = ""
    analysis_result: Dict[str, Any] = field(default_factory=dict)
    
    # Build state
    current_plan: Optional[Dict] = None
    dockerfile_content: Optional[str] = None
    
    # Health and readiness
    health_result: Optional[Dict] = None
    readiness_result: Optional[Dict] = None
    
    # Error handling
    retry_history: List[Dict] = field(default_factory=list)
    reflection: Optional[Dict] = None
    error_message: Optional[str] = None
    error_details: Optional[Dict] = None
    container_logs: str = ""
    
    # Customization
    custom_instructions: str = ""
    
    # External data
    verified_tags: str = ""
    
    # Retry tracking
    retry_count: int = 0
```

### Benefits

1. **Consistency**: Every agent has access to the same information
2. **Type Safety**: IDE autocompletion and type checking
3. **Extensibility**: Add new fields without changing signatures
4. **Debugging**: Easy to inspect what context an agent received

### Usage Example

```python
# Creating context for generator
context = AgentContext(
    file_tree=state["file_tree"],
    file_contents=state["file_contents"],
    analysis_result=state["analysis_result"].dict(),
    current_plan=state["current_plan"].dict(),
    retry_history=[r.dict() for r in state["retry_history"]],
    custom_instructions=prompt_config.generator_instructions or "",
    verified_tags=verified_tags_str,
)

# Generator uses what it needs
dockerfile, project_type, thoughts, usage = generate_dockerfile(context)
```

---

## Data Flow

Understanding how data flows through DockAI helps debug issues and understand where customization applies.

### Complete Data Flow

```mermaid
flowchart TB
    subgraph Input["üì• Input"]
        repo["repo_path = '/path/to/project'"]
    end
    
    subgraph Scan["üìÇ SCAN NODE"]
        scan_desc["‚Ä¢ Walks directory tree<br/>‚Ä¢ Respects .gitignore and .dockerignore<br/>‚Ä¢ Filters out noise (node_modules, .git)"]
        scan_out["file_tree = ['app.py', 'requirements.txt', ...]"]
    end
    
    subgraph Analyze["üß† ANALYZE NODE (AI)"]
        analyze_desc["‚Ä¢ Deduces stack, project_type, base_image<br/>‚Ä¢ Identifies critical files to read"]
        analyze_out["analysis_result = {<br/>  stack: 'Python 3.11 with FastAPI',<br/>  project_type: 'service',<br/>  files_to_read: [...]<br/>}"]
    end
    
    subgraph Read["üìñ READ FILES NODE"]
        read_desc["‚Ä¢ Reads files identified by Analyzer<br/>‚Ä¢ Smart truncation if too large"]
        read_out["file_contents = '=== app.py ===<br/>from fastapi import FastAPI...'"]
    end
    
    subgraph Blueprint["üèóÔ∏è BLUEPRINT NODE (AI)"]
        blueprint_desc["‚Ä¢ Creates strategic build plan<br/>‚Ä¢ Detects health endpoints<br/>‚Ä¢ Identifies startup patterns"]
        blueprint_out["current_plan = {...}<br/>health_result = {...}<br/>readiness_result = {...}"]
    end
    
    subgraph Generate["‚öôÔ∏è GENERATE NODE (AI)"]
        gen_desc["‚Ä¢ Synthesizes context into Dockerfile<br/>‚Ä¢ Verifies base image tags"]
        gen_out["dockerfile_content = 'FROM python:3.11-slim...'"]
    end
    
    subgraph Review["üîí REVIEW NODE (AI)"]
        review_desc["‚Ä¢ Security audit<br/>‚Ä¢ Best practices check"]
        review_out["review_result = {<br/>  is_secure: true,<br/>  issues: []<br/>}"]
    end
    
    subgraph Validate["‚úÖ VALIDATE NODE"]
        validate_desc["‚Ä¢ Hadolint linting<br/>‚Ä¢ Docker build<br/>‚Ä¢ Health check<br/>‚Ä¢ Trivy scan"]
        validate_out["validation_result = {<br/>  success: true,<br/>  image_size: 145MB<br/>}"]
    end
    
    subgraph Output["üì¶ Output"]
        final["Dockerfile written to project"]
    end
    
    Input --> Scan --> Analyze --> Read --> Blueprint --> Generate --> Review --> Validate --> Output
```

---

## The Self-Correcting Loop

The most important architectural feature is the ability to learn from failures.

### Why Self-Correction?

Real-world projects often don't work on the first try:
- Missing build dependencies
- Wrong base image
- Incorrect startup command
- Port mismatch
- Permission issues

A human would read the error, understand the problem, and try again. DockAI does the same.

### How It Works

```mermaid
flowchart TB
    generate["‚öôÔ∏è Generate"]
    validate["‚úÖ Validate"]
    
    generate --> validate
    
    validate -->|"‚úÖ Success"| done["üì¶ Done"]
    validate -->|"‚ùå Failed"| check{"Retries<br/>remaining?"}
    
    check -->|"No"| fail["üí• Fail"]
    check -->|"Yes"| reflect["ü§î Reflect"]
    
    reflect --> decide{"Decide<br/>retry point"}
    
    decide -->|"Minor fix"| generate
    decide -->|"Strategy change"| plan["üìù Plan"]
    decide -->|"Wrong assumptions"| analyze["üß† Analyze"]
    
    plan --> generate
    analyze --> plan
```

### Reflection Logic

The Reflector analyzes errors and decides the appropriate response:

| Error Type | Root Cause | Recommended Action |
|------------|------------|-------------------|
| Missing package | Package manager syntax wrong | Regenerate with fix |
| Missing build tools | Base image too minimal | New plan with different base |
| Wrong runtime | Misdetected stack | Full re-analysis |
| Permission denied | User/ownership issues | Regenerate with correct perms |
| Port conflict | Wrong port exposed | Regenerate with correct port |

### Retry History (Compact Format)

Each retry builds on previous knowledge. The history is kept compact to save tokens (40-50% reduction):

```python
retry_history = [
    {
        "attempt_number": 1,
        "error_type": "dependency_error",
        "error_summary": "pg_config executable not found",  # Truncated to 200 chars
        "what_was_tried": "Used alpine base with pip install",
        "why_it_failed": "psycopg2 needs PostgreSQL headers to compile",
        "lesson_learned": "Alpine needs postgresql-dev for psycopg2",
        "fix_applied": "Add postgresql-dev to build dependencies"
    },
    {
        "attempt_number": 2,
        "error_type": "runtime_error",
        "error_summary": "libpq.so.5 not found",
        "what_was_tried": "Added build deps but used scratch runtime",
        "why_it_failed": "Runtime needs libpq shared library",
        "lesson_learned": "Need libpq5 in final image, not just build stage",
        "fix_applied": "Use debian-slim runtime, install libpq5"
    }
]
```

This history is passed to subsequent agents, so they don't repeat mistakes.

---

## Efficiency Optimizations

DockAI includes several built-in optimizations to reduce generation time and token usage.

### 1. Quality-First Model Selection

The generator always uses a powerful model (e.g., `gpt-4o`) for the first attempt:

```python
# In generate_node - always use powerful model for initial generation
model_name = get_model_for_agent("generator")  # Returns gpt-4o
```

**Rationale**: A 30-50% reduction in retry cycles saves more tokens than using a cheaper model upfront.

### 2. Compact Retry History

Retry history excludes full Dockerfile content and truncates error messages:

- `error_summary`: First 200 characters only
- `fix_applied`: Top 2 specific fixes
- No `dockerfile_content` (already in `previous_dockerfile` state field)

**Token Savings**: 40-50% on retry paths.

### 3. LLM Response Caching

Identical prompts within a run are cached in memory:

```python
# In llm_providers.py
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache
set_llm_cache(InMemoryCache())
```

**When it helps**: Retrying with same error patterns, multiple agents using similar context.

### 4. Early Exit for Scripts

Script projects automatically skip expensive steps:

| Step | Services | Scripts |
|------|----------|---------|
| Security Review | ‚úÖ Full AI review | ‚è≠Ô∏è Skipped |
| Health Checks | ‚úÖ HTTP endpoint verification | ‚è≠Ô∏è Skipped |

**Time Savings**: 30-40% for script projects.

### 5. Smart Registry Tag Fetching

Docker tag lookups use intelligent filtering to minimize token usage:

```python
@lru_cache(maxsize=128)
def get_docker_tags(image: str, target_version: str = None) -> List[str]:
    # 1. If target_version provided (e.g., "18" for Node), use API filter
    # 2. If filter returns nothing (wrong version), fetch ALL tags
    # 3. Cache results for the duration of the run
```

**Registry-Specific Strategies**:

| Registry | Strategy |
|----------|----------|
| **Docker Hub** | `name=` filter for ~100 targeted results, falls back to ALL tags (8,000+) if needed |
| **Quay.io** | Full pagination through all pages (50/page) |
| **GCR/GHCR** | OCI v2 API returns all tags in single request |

**Token Savings**: Only passes ~100 filtered tags to AI instead of thousands.

### 6. RAG Mode for Intelligent Context (v4.0+)

For large projects, DockAI can use Retrieval-Augmented Generation (RAG) to select the most relevant context:

```mermaid
flowchart TB
    subgraph Indexing["üìä Indexing (Once Per Run)"]
        files["All Source Files"] --> ast["AST Parser<br/>(Python, JS, Go)"]
        files --> chunk["Chunking<br/>(400 lines)"]
        ast --> analysis["Code Analysis:<br/>‚Ä¢ Entry points<br/>‚Ä¢ Env vars<br/>‚Ä¢ Ports<br/>‚Ä¢ Frameworks"]
        chunk --> embed["Local Embeddings<br/>(HuggingFace)"]
        embed --> index["Vector Index"]
    end
    
    subgraph Retrieval["üîç Retrieval"]
        query["Query: 'dockerfile dependencies'"]
        query --> search["Semantic Search"]
        index --> search
        analysis --> merge["Merge Results"]
        search --> merge
        merge --> context["Optimized Context<br/>(~50K tokens)"]
    end
    
    context --> llm["LLM Generator"]
```

**Enable with**: `DOCKAI_USE_RAG=true`

**Components**:

| Module | Purpose | Cost |
|--------|---------|------|
| `code_intelligence.py` | AST parsing (Python, JS, Go) + Generic Fallback | FREE (local) |
| `indexer.py` | Semantic search with embeddings | FREE (local HuggingFace model) |
| `context_retriever.py` | Combines AST + search for optimal context | FREE |

**What Code Intelligence Extracts**:

```python
# From a Python file like app.py:
entry_points = ["app.py:main()", "app.py:__main__"]
env_vars = ["DATABASE_URL", "PORT", "DEBUG"]
exposed_ports = [8000, 8080]
frameworks = ["FastAPI", "SQLAlchemy"]
```

**When to Use RAG**:

| Scenario | Recommended Mode |
|----------|-----------------|
| Small project (<50 files) | Standard (read all) |
| Medium project (50-200 files) | Either works |
| Large project (200+ files) | RAG mode |
| Monorepo | RAG mode |

**Performance on GitHub Actions**:

| Repo Size | Indexing Time | Memory |
|-----------|---------------|--------|
| Small | ~5-10s | ~500MB |
| Medium | ~15-30s | ~800MB |
| Large | ~30-60s | ~1.2GB |

---

## Interfaces

DockAI exposes its functionality through three interfaces, all using the same core workflow.

### CLI Interface

**Location**: `src/dockai/cli/main.py`

Built with [Typer](https://typer.tiangolo.com/) for a modern CLI experience:

```python
@app.command()
def build(
    project_path: str = typer.Argument(..., help="Path to project"),
    verbose: bool = typer.Option(False, "--verbose", "-v"),
    no_cache: bool = typer.Option(False, "--no-cache"),
):
    """Generate a Dockerfile for the given project."""
    
    # Create and run workflow
    workflow = create_workflow()
    result = workflow.invoke({
        "repo_path": project_path,
        "retry_count": 0,
        "retry_history": [],
        "token_usage": {},
    })
    
    # Write output
    write_dockerfile(project_path, result["final_dockerfile"])
```

### GitHub Action Interface

**Location**: `action.yml` and `Dockerfile`

The GitHub Action wraps the CLI in a Docker container:

```yaml
# action.yml
inputs:
  openai_api_key:
    description: 'OpenAI API key'
    required: false
  llm_provider:
    description: 'LLM provider'
    default: 'openai'
  # ... more inputs

runs:
  using: 'docker'
  image: 'Dockerfile'
  args:
    - ${{ inputs.project_path }}
```

### MCP Server Interface

**Location**: `src/dockai/core/mcp_server.py`

Implements the [Model Context Protocol](https://modelcontextprotocol.io/) for AI assistants:

```python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("DockAI")

@mcp.tool()
def run_full_workflow(path: str, instructions: str = "") -> str:
    """Run the complete DockAI workflow."""
    
    workflow = create_workflow()
    result = workflow.invoke({
        "repo_path": path,
        "custom_instructions": instructions,
    })
    
    return result["final_dockerfile"]

@mcp.tool()
def analyze_project(path: str) -> dict:
    """Analyze a project without generating Dockerfile."""
    # ... partial workflow
```

---

## Directory Structure

Understanding the codebase organization:

```
src/dockai/
‚îú‚îÄ‚îÄ __init__.py           # Package initialization, version
‚îÇ
‚îú‚îÄ‚îÄ agents/               # AI Agent implementations
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ analyzer.py       # Project analysis agent
‚îÇ   ‚îú‚îÄ‚îÄ generator.py      # Dockerfile generation agent
‚îÇ   ‚îú‚îÄ‚îÄ reviewer.py       # Security review agent
‚îÇ   ‚îî‚îÄ‚îÄ agent_functions.py# Other agents (blueprint architect, reflector, etc.)
‚îÇ
‚îú‚îÄ‚îÄ cli/                  # Command-line interface
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py           # Typer CLI definition
‚îÇ   ‚îî‚îÄ‚îÄ ui.py             # Rich console output
‚îÇ
‚îú‚îÄ‚îÄ core/                 # Core components
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ agent_context.py  # AgentContext dataclass
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py        # Pydantic models for structured output
‚îÇ   ‚îú‚îÄ‚îÄ state.py          # GraphState TypedDict
‚îÇ   ‚îú‚îÄ‚îÄ llm_providers.py  # LLM provider abstraction
‚îÇ   ‚îú‚îÄ‚îÄ mcp_server.py     # MCP server implementation
‚îÇ   ‚îî‚îÄ‚îÄ errors.py         # Custom exception classes
‚îÇ
‚îú‚îÄ‚îÄ utils/                # Utility functions
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ scanner.py        # File tree scanning
‚îÇ   ‚îú‚îÄ‚îÄ file_utils.py     # File reading, truncation
‚îÇ   ‚îú‚îÄ‚îÄ validator.py      # Docker build/run validation
‚îÇ   ‚îú‚îÄ‚îÄ registry.py       # Container registry client
‚îÇ   ‚îú‚îÄ‚îÄ prompts.py        # Prompt loading and configuration
‚îÇ   ‚îú‚îÄ‚îÄ callbacks.py      # LLM callback handlers
‚îÇ   ‚îú‚îÄ‚îÄ rate_limiter.py   # API rate limiting
‚îÇ   ‚îú‚îÄ‚îÄ tracing.py        # OpenTelemetry integration
‚îÇ   ‚îú‚îÄ‚îÄ ollama_docker.py  # Ollama Docker fallback
‚îÇ   ‚îú‚îÄ‚îÄ code_intelligence.py  # AST-based code analysis (v4.0+)
‚îÇ   ‚îú‚îÄ‚îÄ indexer.py        # Semantic search indexing (v4.0+)
‚îÇ   ‚îî‚îÄ‚îÄ context_retriever.py  # Smart context retrieval (v4.0+)
‚îÇ
‚îî‚îÄ‚îÄ workflow/             # LangGraph workflow
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ graph.py          # Workflow graph definition
    ‚îî‚îÄ‚îÄ nodes.py          # Individual node implementations
```

---

## Design Decisions Explained

### Why Multiple Small Agents Instead of One Large Agent?

**Trade-off**: More agents = more API calls, but better results

**Reasoning**:
1. **Focused prompts**: Each agent has a specific, well-defined task
2. **Cost optimization**: Simple tasks use cheaper/faster models
3. **Customization**: Override one agent without affecting others
4. **Debugging**: Clear which agent caused an issue
5. **Parallelization potential**: Future optimization

### Why Validate by Actually Building?

**Trade-off**: Slower, requires Docker, but much more reliable

**Reasoning**:
1. **Catches real issues**: Missing packages, wrong commands, permission problems
2. **No false positives**: If it builds and runs, it works
3. **Health check verification**: Ensures app actually starts
4. **Security scanning**: Real image analysis, not static assumptions

### Why LangGraph Instead of Simple Loops?

**Trade-off**: More complex setup, but better control

**Reasoning**:
1. **Type-safe state**: Compile-time validation of state schema
2. **Conditional edges**: Clean expression of workflow logic
3. **Observability**: Built-in tracing support
4. **Future features**: Checkpointing, streaming, human-in-the-loop

### Why Support Multiple LLM Providers?

**Trade-off**: More code to maintain, but broader accessibility

**Reasoning**:
1. **Cost optimization**: Different providers for different budgets
2. **Enterprise requirements**: Some organizations mandate specific providers
3. **Privacy**: Ollama enables fully local processing
4. **Resilience**: Fallback options if one provider is down

### Why Structured Output (Pydantic)?

**Trade-off**: More rigid, but much more reliable

**Reasoning**:
1. **Type safety**: Guaranteed fields and types
2. **Validation**: Invalid responses caught immediately
3. **Documentation**: Schema serves as documentation
4. **IDE support**: Autocompletion and type hints

---

## Next Steps

Now that you understand the architecture:

- **[API Reference](./api-reference.md)**: Detailed function documentation for extending DockAI
- **[Customization](./customization.md)**: How to tune agents for your needs
- **[Configuration](./configuration.md)**: All configuration options explained
- **[MCP Server](./mcp-server.md)**: Using the MCP interface with AI assistants
