# =============================================================================
# DockAI Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values:
#   cp .env.example .env
#
# For GitHub Actions, set these as repository secrets or use action inputs.
# =============================================================================


# =============================================================================
# LLM PROVIDER CONFIGURATION
# =============================================================================
# DockAI supports multiple LLM providers. Set ONE of the following API keys.

# LLM Provider to use (default: openai)
# Options: openai, azure, gemini, anthropic, ollama
DOCKAI_LLM_PROVIDER=openai

# OpenAI API Key (required if using openai provider)
OPENAI_API_KEY=sk-your-api-key-here

# Google Gemini API Key (required if using gemini provider)
GOOGLE_API_KEY=your-google-api-key

# Anthropic API Key (required if using anthropic provider)
ANTHROPIC_API_KEY=your-anthropic-api-key

# Azure OpenAI (required if using azure provider)
AZURE_OPENAI_API_KEY=your-azure-openai-key
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-02-15-preview

# Google Cloud Project ID (optional - for Vertex AI integration)
GOOGLE_CLOUD_PROJECT=your-gcp-project-id

# Ollama (required if using ollama provider)
OLLAMA_BASE_URL=http://localhost:11434


# =============================================================================
# AI MODEL CONFIGURATION
# =============================================================================
# Per-agent model configuration (optional - uses smart defaults if not set)
#
# MIXED PROVIDER SUPPORT:
# You can mix providers by prefixing the model name with "provider/".
# Example: DOCKAI_MODEL_ANALYZER=openai/gpt-4o-mini (overrides default provider)



# Per-agent model configuration (advanced - overrides the above)
# Example: Use a specific model for analysis but default provider for everything else
# DOCKAI_LLM_PROVIDER=ollama
# DOCKAI_MODEL_ANALYZER=openai/gpt-4o-mini

# Defaults (can be overridden with any supported model)
# DOCKAI_MODEL_ANALYZER=gpt-4o-mini
# DOCKAI_MODEL_BLUEPRINT=gpt-4o
# DOCKAI_MODEL_GENERATOR=gpt-4o
# DOCKAI_MODEL_GENERATOR_ITERATIVE=gpt-4o
# DOCKAI_MODEL_REVIEWER=gpt-4o-mini
# DOCKAI_MODEL_REFLECTOR=gpt-4o
# DOCKAI_MODEL_ERROR_ANALYZER=gpt-4o-mini
# DOCKAI_MODEL_ITERATIVE_IMPROVER=gpt-4o


# =============================================================================
# GENERATION SETTINGS
# =============================================================================

# Maximum retry attempts if Dockerfile validation fails (default: 3)
MAX_RETRIES=3


# =============================================================================
# VALIDATION SETTINGS
# =============================================================================

# Memory limit for container validation sandbox (default: 512m)
# Increase for memory-intensive apps (e.g., Java: 1g, ML: 2g)
DOCKAI_VALIDATION_MEMORY=512m

# CPU limit for container validation (default: 1.0)
DOCKAI_VALIDATION_CPUS=1.0

# Maximum processes for container validation (default: 100)
DOCKAI_VALIDATION_PIDS=100

# Maximum image size in MB, 0 to disable (default: 500)
DOCKAI_MAX_IMAGE_SIZE_MB=500

# Skip health check during validation (default: false)
# Set to true for CLIs, scripts, or non-server applications
DOCKAI_SKIP_HEALTH_CHECK=false


# =============================================================================
# FILE ANALYSIS SETTINGS
# =============================================================================

# Enable smart truncation of large files (default: false)
# When false, files are read in full regardless of size
# Auto-enables if total content exceeds DOCKAI_TOKEN_LIMIT
DOCKAI_TRUNCATION_ENABLED=false

# Token limit for auto-truncation (default: 100000)
# If total file content exceeds this, truncation auto-enables
# Approximate: 1 token â‰ˆ 4 characters
DOCKAI_TOKEN_LIMIT=100000

# Maximum characters to read per file when truncating (default: 200000)
DOCKAI_MAX_FILE_CHARS=200000

# Maximum lines to read per file when truncating (default: 5000)
DOCKAI_MAX_FILE_LINES=5000


# =============================================================================
# CONTEXT RETRIEVAL (RAG) SETTINGS
# =============================================================================
# RAG (Retrieval-Augmented Generation) uses semantic search for intelligent
# context selection. It is the DEFAULT and RECOMMENDED mode in v4.0.

# Enable RAG mode for intelligent context retrieval (default: true in v4.0)
# When enabled, uses semantic search + AST analysis to select relevant files
# RAG dependencies (sentence-transformers, numpy) are now included by default
DOCKAI_USE_RAG=true

# Read all source files instead of just analyzer-selected (default: true)
# Only applicable when RAG is disabled (not recommended)
DOCKAI_READ_ALL_FILES=true

# Embedding model for RAG (default: all-MiniLM-L6-v2)
# Uses HuggingFace sentence-transformers (runs locally, no API cost)
# DOCKAI_EMBEDDING_MODEL=all-MiniLM-L6-v2

# HuggingFace tokenizers parallelism (automatically set to false by DockAI)
# DockAI automatically sets TOKENIZERS_PARALLELISM=false to suppress fork warnings
# Only override this if you specifically need parallelism enabled
# TOKENIZERS_PARALLELISM=true


# =============================================================================
# SECURITY SETTINGS
# =============================================================================

# Skip Hadolint Dockerfile linting (default: false)
# Hadolint checks for best practices in Dockerfiles
DOCKAI_SKIP_HADOLINT=false

# Skip Trivy security scan entirely (default: false)
# Not recommended for production
DOCKAI_SKIP_SECURITY_SCAN=false

# Strict security mode (default: false)
# When true, fails on ANY HIGH/CRITICAL vulnerabilities
# When false, only fails on critical; warns on others
DOCKAI_STRICT_SECURITY=false

# Skip AI security review for faster generation (default: false)
# Automatically skipped for script projects (single-run, lower risk)
# Only enable manually for trusted environments
DOCKAI_SKIP_SECURITY_REVIEW=false


# =============================================================================
# EFFICIENCY OPTIMIZATIONS
# =============================================================================

# Enable in-memory LLM response caching (default: true)
# Caches identical prompts within a single run to avoid repeated API calls
# Useful when retrying with the same error patterns
DOCKAI_LLM_CACHING=true


# =============================================================================
# OBSERVABILITY & TRACING (Optional)
# =============================================================================
# OpenTelemetry distributed tracing for debugging and monitoring.
# Useful for enterprise deployments or troubleshooting.

# Enable metrics collection (default: false)
# Collects workflow metrics for analysis
DOCKAI_COLLECT_METRICS=false

# Enable OpenTelemetry tracing (default: false)
# When enabled, spans are created for each workflow node
DOCKAI_ENABLE_TRACING=false

# Tracing exporter type (default: console)
# Options: console (stdout), otlp (OTLP protocol for Jaeger/Tempo/Datadog)
DOCKAI_TRACING_EXPORTER=console

# OTLP endpoint URL (only used when DOCKAI_TRACING_EXPORTER=otlp)
# Examples: http://localhost:4317 (Jaeger), http://tempo:4317 (Grafana Tempo)
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

# Service name for traces (default: dockai)
OTEL_SERVICE_NAME=dockai


# =============================================================================
# LANGSMITH OBSERVABILITY (Optional)
# =============================================================================
# LangSmith provides specialized observability for LLM applications.
# See: https://smith.langchain.com/

# Enable LangSmith tracing (default: false)
LANGCHAIN_TRACING_V2=false

# LangSmith API Key (required if tracing is enabled)
LANGCHAIN_API_KEY=ls__...

# LangSmith Project Name (default: default)
LANGCHAIN_PROJECT=dockai

# LangSmith Endpoint (default: https://api.smith.langchain.com)
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com


# =============================================================================
# CUSTOM INSTRUCTIONS (Optional)
# =============================================================================
# Instructions are APPENDED to the agent's default prompt.
# Use these to add project-specific guidance.
#
# Alternative: Use a .dockai file in your project root (see .dockai.example)
# =============================================================================

# Project analyzer - discovers and analyzes project structure
DOCKAI_ANALYZER_INSTRUCTIONS=

# Blueprint agent - plans build strategy and runtime config
DOCKAI_BLUEPRINT_INSTRUCTIONS=

# Dockerfile generator - creates the Dockerfile
DOCKAI_GENERATOR_INSTRUCTIONS=

# Iterative generator - fixes Dockerfile after failures
DOCKAI_GENERATOR_ITERATIVE_INSTRUCTIONS=

# Security reviewer - reviews for vulnerabilities
DOCKAI_REVIEWER_INSTRUCTIONS=

# Failure reflector - analyzes why generation failed
DOCKAI_REFLECTOR_INSTRUCTIONS=

# Error analyzer - classifies and diagnoses errors
DOCKAI_ERROR_ANALYZER_INSTRUCTIONS=

# Iterative improver - applies specific fixes
DOCKAI_ITERATIVE_IMPROVER_INSTRUCTIONS=


# =============================================================================
# CUSTOM PROMPTS (Advanced)
# =============================================================================
# Prompts COMPLETELY REPLACE the agent's default prompt.
# Use with caution - defaults are designed to work with any tech stack.
#
# Alternative: Use a .dockai file in your project root (see .dockai.example)
# =============================================================================

DOCKAI_PROMPT_ANALYZER=
DOCKAI_PROMPT_BLUEPRINT=
DOCKAI_PROMPT_GENERATOR=
DOCKAI_PROMPT_GENERATOR_ITERATIVE=
DOCKAI_PROMPT_REVIEWER=
DOCKAI_PROMPT_REFLECTOR=
DOCKAI_PROMPT_ERROR_ANALYZER=
DOCKAI_PROMPT_ITERATIVE_IMPROVER=
